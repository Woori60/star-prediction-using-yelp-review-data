{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ㄴㄴ.ipynb의 사본","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"F0lWIxXTFudj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"0e966f01-8539-4f24-f24d-0003bdf4ecbc","executionInfo":{"status":"ok","timestamp":1566895681977,"user_tz":-540,"elapsed":598,"user":{"displayName":"woori team","photoUrl":"","userId":"00533524483208423131"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xUUnnyrlF0WR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":197},"outputId":"b5200cfb-db1d-499f-a9f5-6c8d5d0850b5","executionInfo":{"status":"ok","timestamp":1566896008181,"user_tz":-540,"elapsed":299570,"user":{"displayName":"woori team","photoUrl":"","userId":"00533524483208423131"}}},"source":["import json\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tag import pos_tag\n","import re\n","\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","\n","n=WordNetLemmatizer()\n","\n","tx1 = ''\n","tx2 = ''\n","\n","#print('***original text***\\n')\n","with open('/content/drive/My Drive/소스코드/3만개 리뷰용/restaurant_30000.json', 'rt', encoding='UTF-8') as f:\n","    for line in f:\n","        lineobj = json.loads(line)\n","        tx1 = lineobj['text']       \n","        tx3 = tx1.lower()\n","        word_tokens = word_tokenize(tx3)\n","\n","        result = []\n","        words = []\n","\n","        #print(word_tokens)\n","\n","        #print('\\n\\n***lemmatize***\\n')\n","        \n","        tag_l = pos_tag(word_tokens)\n","\n","        for w in tag_l:\n","            if w[1][:2] == 'VB':\n","                words.append(n.lemmatize(w[0], 'v'))\n","            else:\n","                words.append(n.lemmatize(w[0]))\n","        #print(words)\n","        \n","        stop_words = set(stopwords.words('english'))\n","        stop_words.update(['\\'re', '\\'d', '\\'t', '\\'ll', '\\'ve', '\\'s','\\'m', '!', '.', ',', '/', '?', '\\\"', '@', '%', '&', '*', '=','(',')','{','}', '-', '--', 'u', '...','$', '#', '*', '@', ':', ';', '[',']','~']) \n","        stop_words.remove('not')\n","        \n","        for w in words:\n","            if w == 'n\\'t':\n","                w = 'not'\n","            if w not in stop_words:\n","                a = re.sub('[^a-zA-Z ]', '', w)\n","                if a != '':\n","                    result.append(a) #result에 불용어 제거된 tokens 존재\n","\n","        #print('\\n\\n***result***\\n', result)\n","        \n","        with open('/content/drive/My Drive/소스코드/30000_review_tokens.txt', 'a', encoding='UTF-8') as mf:\n","            mf.write('[\\n')\n","            mf.write('\\n'.join(result))\n","            mf.write('\\n]\\n')\n","            \n","\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]}]}