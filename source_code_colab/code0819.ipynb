{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"code0819.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"84GhNfztz2rc","colab_type":"code","outputId":"ba4bafce-e150-4b48-b657-e975e5bbb679","executionInfo":{"status":"error","timestamp":1566291513539,"user_tz":-540,"elapsed":585,"user":{"displayName":"woori team","photoUrl":"","userId":"00533524483208423131"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["from numpy import array\n","from keras.preprocessing.text import one_hot\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers.embeddings import Embedding\n","from nltk.tokenize import word_tokenize\n","#from nltk.tokenize import word_tokenize\n","import re\n","import nltk\n","from keras.layers import Embedding, LSTM\n","from keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout, Conv2D, MaxPooling2D\n","\n","import json\n","\n","sentiment_label = []\n","test_label=[]\n","\n","'''\n","review_list = []\n","with open('/content/drive/My Drive/dataset/전처리 전 json/tempe_3000.json', 'rt', -1, encoding='UTF-8') as f:\n","    for line in f:\n","        lineobj = json.loads(line)\n","        review_txt = re.sub('[^a-zA-Z ]', '', lineobj['text'])\n","        review_txt = review_txt.lower()\n","        review_list.append(review_txt)\n","        \n","print(review_list)\n","            \n","\n","# test data\n","test_list = []\n","with open('/content/drive/My Drive/dataset/전처리 전 json/vegas_3000.json', 'rt', -1, encoding='UTF-8') as f:\n","    for line in f:\n","        lineobj = json.loads(line)\n","        test_txt = re.sub('[^a-zA-Z ]', '', lineobj['text'])\n","        test_txt = test_txt.lower()\n","        test_list.append(test_txt)\n","'''\n","\n","\n","result = []\n","review_list = []\n","with open('/content/drive/My Drive/dataset/token_list/tempe_3000_tokens.txt', 'rt', encoding='UTF-8') as mf:\n","    while True:\n","        temp = mf.readline()\n","        if not temp: break\n","        if temp == '[\\n':\n","            continue\n","        elif temp == ']\\n':\n","            review_list.append(result)\n","            result = []\n","            continue\n","        else:\n","            l = len(temp)\n","            word = temp[0:l - 1]\n","            result.append(word)\n","\n","result = []\n","test_list = []\n","with open('/content/drive/My Drive/dataset/token_list/tempe_3000_tokens.txt', 'rt', encoding='UTF-8') as mf:\n","    while True:\n","        temp = mf.readline()\n","        if not temp: break\n","        if temp == '[\\n':\n","            continue\n","        elif temp == ']\\n':\n","            test_list.append(result)\n","            result = []\n","            continue\n","        else:\n","            l = len(temp)\n","            word = temp[0:l - 1]\n","            result.append(word)\n","\n","with open('/content/drive/My Drive/dataset/전처리 전 json/tempe_3000.json', 'rt', -1, encoding='UTF-8') as f:\n","    for line in f:\n","        lineobj = json.loads(line)\n","        star = lineobj['stars']\n","        if star >= 4:\n","            sentiment_label.append('2')\n","        elif star == 3:\n","            sentiment_label.append('1')\n","        else:\n","            sentiment_label.append('0')\n","            \n","sentiment_label = array(sentiment_label)\n","\n","# test data\n","with open('/content/drive/My Drive/dataset/전처리 전 json/vegas_3000.json', 'rt', -1, encoding='UTF-8') as f:\n","    for line in f:\n","        lineobj = json.loads(line)\n","        star = lineobj['stars']\n","        if star >= 4:\n","            test_label.append('2')\n","        elif star == 3:\n","            test_label.append('1')\n","        else:\n","            test_label.append('0')\n","            \n","test_label = array(test_label)\n","\n","\"\"\"           \n","word_list = []\n","for i in range(len(result_c)):\n","  for j in range(len(result_c[i])):\n","    word_list.append(result_c[i][j])\n","    \n","    \n","print(word_list)\n","print('******')\n","print(len(word_list))\n","unique_words = set(word_list)\n","print(len(unique_words))  #9422\n","\n","word_num = len(unique_words)\n","vocab_length = word_num+5\n","\"\"\"   \n","\n","#embedded_sentences = [one_hot(sent, vocab_length) for sent in result_c]\n","#print(embedded_sentences)\n","\"\"\"\n","embedded_sentences = []\n","for i in range(len(result_c)):\n","  for sent in result_c[i]:\n","    embedded_sentences.append(one_hot(sent, vocab_length))\n","    embedded_sentences[i].append(temp[i])\n","print(embedded_sentences)\n","\"\"\"\n","'''\n","all_words = []\n","for sent in review_list:\n","    tokenize_word = word_tokenize(sent)\n","    for word in tokenize_word:\n","        all_words.append(word)\n","#print(all_words)\n","unique_words = set(all_words)\n","print(len(unique_words))\n","'''\n","all_words = []\n","for sent in review_list:\n","  for w in sent:\n","    all_words.append(w)\n","#print(all_words)\n","unique_words = set(all_words)\n","#print(len(unique_words))\n","vocab_length = len(unique_words)+5\n","'''\n","# test data\n","test_words = []\n","for sent in test_list:\n","    tokenize_word_test = word_tokenize(sent)\n","    for word in tokenize_word_test:\n","        test_words.append(word)\n","#print(all_words)\n","unique_words_test = set(test_words)\n","test_length = len(unique_words_test)+5 # 5 안 더하면 어떤지 확인하기\n","'''\n","test_words = []\n","for sent in test_list:\n","  for word in sent:\n","    test_words.append(word)\n","#print(all_words)\n","unique_words_test = set(test_words)\n","test_length = len(unique_words_test)+5 # 5 안 더하면 어떤지 확인하기\n","\n","embedded_sentences = []\n","embedded_sentences_test = []\n","for sent in review_list:\n","  for w in sent:\n","    oh = one_hot(w, vocab_length)\n","    embedded_sentences.append(oh)\n","    \n","#print(embedded_sentences)\n","for sent in test_list:\n","  for t in sent:\n","    oh = one_hot(t, vocab_length)\n","    embedded_sentences_test.append(oh)\n","    \n","#embedded_sentences = [one_hot(sent, vocab_length) for sent in review_list]\n","#embedded_sentences_test = [one_hot(sent, test_length) for sent in test_list]\n","#print(embedded_sentences)\n","\n","tok_list = []\n","tok_test_list = []\n","for word in review_list:\n","  for w in word:\n","    tok_list.append(w)\n","\n","for word in test_list:\n","  for w in word:\n","    tok_test_list.append(w)\n","\n","word_count = lambda sentence: len(word_tokenize(sentence))\n","longest_sentence = max(tok_list, key=word_count)\n","length_long_sentence = len(word_tokenize(longest_sentence))\n","\n","padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n","#print(padded_sentences)\n","\n","# test data\n","word_count_test = lambda sentence_test: len(word_tokenize(sentence_test))\n","longest_sentence_test = max(tok_test_list, key=word_count_test)\n","length_long_sentence_test = len(word_tokenize(longest_sentence_test))\n","\n","padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n","padded_sentences_test = pad_sequences(embedded_sentences_test, length_long_sentence_test, padding='post')\n","\n","model = Sequential()\n","model.add(Embedding(vocab_length, 20, input_length=length_long_sentence))\n","#model.add(Flatten())\n","model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(1, activation='softmax'))\n","\"\"\"\n","model = Sequential()\n","model.add(Embedding(vocab_length, 100, input_length=length_long_sentence))\n","model.add(Conv2D(filters=128, kernel_size=(5,5), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2,2))\n","model.add(Conv2D(filters=128, kernel_size=(5,5), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","model.add(Conv2D(filters=128, kernel_size=(5,5), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","model.add(Dropout(0.4))\n","model.add(Flatten())\n","model.add(Dense(256, activation='relu'))\n","model.add(Dense(128, activation='relu'))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","\"\"\"\n","\n","#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n","print(model.summary())\n","result = model.fit(padded_sentences, sentiment_label, epochs=100, batch_size=100 ,verbose=1)\n","\n","loss, accuracy = model.evaluate(padded_sentences_test, test_label, verbose=1)\n","#loss, accuracy = model.evaluate(padded_sentences, sentiment_label, verbose=1)\n","print('Accuracy: %f' % (accuracy*100))\n","\n","# 5. 학습과정 살펴보기\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","fig, loss_ax = plt.subplots()\n","\n","acc_ax = loss_ax.twinx()\n","\n","loss_ax.plot(result.history['loss'], 'y', label='train loss')\n","loss_ax.plot(result.history['val_loss'], 'r', label='val loss')\n","loss_ax.set_ylim([-0.2, 1.2])\n","\n","acc_ax.plot(result.history['acc'], 'b', label='train acc')\n","acc_ax.plot(result.history['val_acc'], 'g', label='val acc')\n","acc_ax.set_ylim([-0.2, 1.2])\n","\n","loss_ax.set_xlabel('epoch')\n","loss_ax.set_ylabel('loss')\n","acc_ax.set_ylabel('accuracy')\n","\n","loss_ax.legend(loc='upper left')\n","acc_ax.legend(loc='lower left')\n","\n","plt.show()\n"],"execution_count":0,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-ecc80f14e2a0>\"\u001b[0;36m, line \u001b[0;32m220\u001b[0m\n\u001b[0;31m    model.add(Conv2D(filters=128, kernel_size=(5,5), activation='relu'))\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"id":"5mqzR4RPBISd","colab_type":"code","outputId":"c0a95c09-f315-4499-ed48-235da057c624","executionInfo":{"status":"ok","timestamp":1566287646439,"user_tz":-540,"elapsed":17479,"user":{"displayName":"woori team","photoUrl":"","userId":"00533524483208423131"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b1SgN9EzKbRI","colab_type":"code","outputId":"6dc413f6-1d74-445a-834a-e4ad330bfec8","executionInfo":{"status":"ok","timestamp":1566287709510,"user_tz":-540,"elapsed":1098,"user":{"displayName":"woori team","photoUrl":"","userId":"00533524483208423131"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["nltk.download('punkt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"83Tf34Dg1_Iz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"olOaum9Wz6CD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-3FXOht0dG1","colab_type":"code","outputId":"3de57a06-c20e-418c-fd8a-48affc175a1a","executionInfo":{"status":"ok","timestamp":1566270598245,"user_tz":-540,"elapsed":21986,"user":{"displayName":"woori team","photoUrl":"","userId":"00533524483208423131"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1qeW-jhyAfzv","colab_type":"code","outputId":"f0f7e6ab-281d-4d5b-fecf-a7832caee5ce","executionInfo":{"status":"ok","timestamp":1566270682401,"user_tz":-540,"elapsed":1037,"user":{"displayName":"woori team","photoUrl":"","userId":"00533524483208423131"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"_CmO0egrAnhQ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}